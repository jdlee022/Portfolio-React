{
  "title": "Machine Learning Basics: Linear Regression",
  "textMD": "# Machine Learning Basics: Linear Regression\n\nIf you're reading this blog post you probably already have an understanding of machine learning, statistics, or both. For those who are unfamiliar, **machine learning** is a field of computer science that focuses on creating algorithms that give computers the ability to learn without being explicitly programmed. These algorithms are typically classified as being **supervised** or **unsupervised**. Supervised algorithms need to be trained with data sets that provide an output for each of the sample inputs, whereas unsupervised algorithms do not require outcome data to be trained.\n\n## Linear regression\n\n**Linear Regression** is a form of supervised learning, and it is one of the most common (and most simple) machine learning algorithms that you will see. It is also one of the most import techniques you will learn in the field of statistics. In short, linear regression is a model that assumes a linear relationship between input variable(s) and a single output variable.\n\nA linear equation assigns one coefficient (scale factor) for each input value. Here we will represent these coefficients with the Greek letter Beta (B). There is always an additional coefficient (degree of freedom) that is added to the equation that is often referred to as the bias coefficient. In a **simple regression** equation, the model would be represented as:\n\n`y = B0 + B1*x`\n\nWe often refer to the number of inputs used in the equation as the complexity of the model, so we refer to the above equation with only one input as a simple regression because it is the least complex linear regression possible.\n\nWhen analyzing a linear regression model it is important to note that as a coefficient approaches zero, it is essentially telling us that the corresponding input value has no effect on the output. The larger a coefficient relative to the other coefficients in the equation, the more impact the corresponding input has on the output. The process of learning a linear regression model involves estimating the values of these coefficients based on a provided set of data.\n\n## Data preparation\n\nIt is very important that the training data that is used for a linear regression model follows certain criteria in order for it to produce the best results. For example, it is crucial that the relation between the input and output is linear. Sometimes it is necessary to transform data in order for it to fit the model (eg using a log transform for an exponential relationship).\n\nRemoving noisy data can also significantly improve the accuracy of a linear regression model, especially when removing outliers in the output variable. Some learning techniques can be memory intensive so it is always helpful to remove as much noisy data as possible. \n\nLastly, training data that has a normal distribution can also significantly improve the speed and accuracy when learning a linear regression model, so when possible it is best to transform the data to look more like a normal distribution. There are certainly other ways to improve your training data, but the characteristics listed above are among the most important.\n\n## How do we find these coefficients?\n\nSince the concept of linear regression has been around for nearly 200 years there are a lot of different proven methods to approach this problem. Here we will look at the **gradient descent** technique since it is commonly used in machine learning algorithms. This technique initially assigns random values to the coefficients and iteratively minimizes error with training data.\n\nThe procedure goes as follows: select a random point on the function, calculate the derivative of the cost of the coefficients (cost is evaluated by plugging the coefficients into the function),\n\n`cost = f(coefficient)`\n\n`delta = derivative(cost)`\n\n and taking a step \"downhill\" by updating the coefficient values based on the slope givin by the derivative. Before we do that we must choose the learning rate parameter(alpha) which determines the size of the step.\n\n ```coefficient = coefficient - (alpha * delta)```\n\n We repeat these steps until the cost is 0 or close to it. Now we finally have our coefficients. Once we have the coefficients for our linear regression model we are able to make output predictions for a provided input.\n\n ## Conclusion\n\nThis was just a brief overview of the concept of linear regression but we covered enough to get an idea for how big of a role optimization plays in machine learning. To get a deeper understanding of these concepts and what role they play in machine learning then I highly recommend checking out Jason Brownlee's [Machine Learning Mastery](https://machinelearningmastery.com/start-here/) series.",
  "excerpt": "Here I give a brief overview of machine learning and one of the most commonly used algorithms in the field -- linear regression. These notes were collected from various sources across the web, most notably from Jason Brownlee's excellent Machine Learning Mastery series.",
  "date": "Nov. 27th, 2017",
  "featured": false,
  "tags": "machine learning, algorithms"
}